import logging
from openai_interface import OpenAIInterface


######################################################
# Ideas for further expansion:
# TODO: Writing study guides, questions, and answers to a file
# TODO: Loading from file to resume later
# TODO: short answer questions (Use GPT to judge if your answer is correct!)
# TODO: Settings (such as skip explanation if you got it correct, etc)
######################################################



def print_welcome_message():
    welcome_message = \
    f"""Welcome to the handy dandy test prep tool!

What would you like to do?
    
    1. New Test
    2. Load from file (Challenge problem for you to finish!)
    
"""

    print(welcome_message)


def initialize_test_agent(llm_interface, test_info):
    conversation = []

    system_prompt = \
        f"""You are a study helper who will assist a student studying for a test. Details about the subject matter and expectations are as follows: {test_info}.
        Ensure you are thorough in explaining your reasoning at all times."""
    
    add_to_conversation(conversation, system_prompt, 'system')

    return conversation


def create_study_guide(llm_interface: OpenAIInterface, conversation: list):
    study_guide_prompt = f"""Create a study guide based on the information about the test"""

    add_to_conversation(conversation, study_guide_prompt, 'user')

    response = llm_interface.predict_text(conversation, max_tokens=400, prompt_as_chat=True)

    add_to_conversation(conversation, response, 'assistant')

    return response


def create_multiple_choice(llm_interface: OpenAIInterface, conversation: list):
    print('Thinking of a question...')
    mcq_prompt = f"""Please create a multiple choice question based on the test information. Format the response as follows: 'Question: [question_content] Choices: A) [choice1] B) [choice2] ... Answer: [answer_key]'"""

    add_to_conversation(conversation, mcq_prompt, 'user')

    response = llm_interface.predict_text(conversation, max_tokens=400, prompt_as_chat=True)

    add_to_conversation(conversation, response, 'assistant')

    # Extract the question, choices, and answer key
    question, choices, answer_key = parse_mcq(response)
    
    return question, choices, answer_key

def parse_mcq(mcq_text: str):
    try:
        question = mcq_text.split('Choices:')[0].strip().replace('Question: ', '')
        choices_text = mcq_text.split('Choices:')[1].split('Answer:')[0].strip()
        choices = choices_text.split('\n')
        answer_key = mcq_text.split('Answer: ')[1].strip()
    except IndexError as e:
        logging.error(f"Failed to parse MCQ text: {mcq_text}")
        logging.error(str(e))
        return None, None, None
    
    return question, choices, answer_key


def explain_answer(llm_interface: OpenAIInterface, conversation: list, question: str, correct_answer: str, student_answer: str):
    """
    Prompts the OpenAI API to explain why the correct answer is what it is, 
    and if the student's answer was incorrect, to explain why that answer was wrong.
    
    Parameters:
        llm_interface: The interface object to interact with the OpenAI API.
        conversation: The current conversation history to be sent to the OpenAI API.
        question: The question that was asked.
        correct_answer: The correct answer key.
        student_answer: The answer that the student chose.
        
    Returns:
        explanation: The explanation text generated by the OpenAI API.
    """
    print('Thinking of an explanation...')
    if student_answer.lower() == correct_answer.lower():
        prompt = f"The student answered '{student_answer}' to the question '{question}', which is correct. Please explain why this answer is correct."
    else:
        prompt = f"The student answered '{student_answer}' to the question '{question}', which is incorrect. The correct answer is '{correct_answer}'. Please explain why the student's answer is incorrect and why the correct answer is '{correct_answer}'."
    
    add_to_conversation(conversation, prompt, 'user')

    response = llm_interface.predict_text(conversation, max_tokens=400, prompt_as_chat=True)

    add_to_conversation(conversation, response, 'assistant')
    
    return response



def read_input():
    input_text = ''
    print("Enter text. Press Ctrl+D (or Ctrl+Z on Windows) to stop.")
    
    try:
        while True:
            line = input()
            input_text += line + '\n'
    except EOFError:
        pass

    return input_text


def add_to_conversation(conversation: list, entry: str, role: str):
    """
        Adds a string to a ChatGPT conversation.
    """
    if role not in ['system', 'user', 'assistant']:
        logging.error(f"Role {role} not 'system', 'user', or 'assistant'")

    conversation.append({'role': role, 'content': entry})


if __name__ == '__main__':

    personal_api_key = ''
    llm_interface = OpenAIInterface(api_key=personal_api_key)

    print_welcome_message()

    choice = input()

    # TODO: What happens if you pick something that isn't an option?

    if '1' in choice:
        print('Give the LLM test guidelines such as content, level learning objectives, a study guide, etc.')

        test_info = read_input()
        
        history = initialize_test_agent(llm_interface, test_info)

        format_string = \
        """How would you like to prep?
1. Make a study guide
2. Multiple Choice Questions
3. Short answer questions (Challenge Problem for you to finish)

        """

        choice = input(format_string)

        if '1' in choice:
            print('Thinking...')
            print(create_study_guide(llm_interface, history))
        if '2' in choice:
            while True: 
                question, choices, answer_key = create_multiple_choice(llm_interface, history)
                
                if question is None or choices is None or answer_key is None:
                    print('Failed to generate a valid multiple choice question. Please try again.\n')
                    continue
                
                print(question)
                for ch in choices:
                    print(f'{ch}\n')

                
                user_answer = input('Your answer: ')
                
                if user_answer.lower() == answer_key.lower()[0]:
                    print('Correct!\n')
                else:
                    print(f'Incorrect. The correct answer is {answer_key}.\n')
                    
                explanation = explain_answer(llm_interface, history, question, answer_key, user_answer)
                print('Explanation:', explanation, '\n')
                
                # Ask if the user wants to try another question
                continue_choice = input('Would you like another question? (yes/no): ')
                if continue_choice.lower() == 'no':
                    break
